{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21111508",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from proj1_helpers import *\n",
    "from implementations import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5c38df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data/train.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6599baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "y, x, ids = load_csv_data(data_path, sub_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49337e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tx, mean_x, std_x = standardize(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "211993b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SGD parameters\n",
    "initial_w = np.zeros((x.shape[1]))\n",
    "max_iters = 100\n",
    "gamma = 0.01\n",
    "lambda_ = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f9ef59c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/99): loss=0.5000000000\n",
      "Gradient Descent(1/99): loss=0.4401420042\n",
      "Gradient Descent(2/99): loss=0.4367786984\n",
      "Gradient Descent(3/99): loss=0.4349683956\n",
      "Gradient Descent(4/99): loss=0.4333423494\n",
      "Gradient Descent(5/99): loss=0.4318501655\n",
      "Gradient Descent(6/99): loss=0.4304762522\n",
      "Gradient Descent(7/99): loss=0.4292078895\n",
      "Gradient Descent(8/99): loss=0.4280341066\n",
      "Gradient Descent(9/99): loss=0.4269453800\n",
      "Gradient Descent(10/99): loss=0.4259334015\n",
      "Gradient Descent(11/99): loss=0.4249908885\n",
      "Gradient Descent(12/99): loss=0.4241114286\n",
      "Gradient Descent(13/99): loss=0.4232893508\n",
      "Gradient Descent(14/99): loss=0.4225196188\n",
      "Gradient Descent(15/99): loss=0.4217977432\n",
      "Gradient Descent(16/99): loss=0.4211197069\n",
      "Gradient Descent(17/99): loss=0.4204819036\n",
      "Gradient Descent(18/99): loss=0.4198810859\n",
      "Gradient Descent(19/99): loss=0.4193143206\n",
      "Gradient Descent(20/99): loss=0.4187789520\n",
      "Gradient Descent(21/99): loss=0.4182725695\n",
      "Gradient Descent(22/99): loss=0.4177929803\n",
      "Gradient Descent(23/99): loss=0.4173381857\n",
      "Gradient Descent(24/99): loss=0.4169063611\n",
      "Gradient Descent(25/99): loss=0.4164958376\n",
      "Gradient Descent(26/99): loss=0.4161050868\n",
      "Gradient Descent(27/99): loss=0.4157327071\n",
      "Gradient Descent(28/99): loss=0.4153774116\n",
      "Gradient Descent(29/99): loss=0.4150380176\n",
      "Gradient Descent(30/99): loss=0.4147134369\n",
      "Gradient Descent(31/99): loss=0.4144026675\n",
      "Gradient Descent(32/99): loss=0.4141047860\n",
      "Gradient Descent(33/99): loss=0.4138189408\n",
      "Gradient Descent(34/99): loss=0.4135443460\n",
      "Gradient Descent(35/99): loss=0.4132802759\n",
      "Gradient Descent(36/99): loss=0.4130260597\n",
      "Gradient Descent(37/99): loss=0.4127810777\n",
      "Gradient Descent(38/99): loss=0.4125447561\n",
      "Gradient Descent(39/99): loss=0.4123165644\n",
      "Gradient Descent(40/99): loss=0.4120960110\n",
      "Gradient Descent(41/99): loss=0.4118826408\n",
      "Gradient Descent(42/99): loss=0.4116760316\n",
      "Gradient Descent(43/99): loss=0.4114757924\n",
      "Gradient Descent(44/99): loss=0.4112815600\n",
      "Gradient Descent(45/99): loss=0.4110929977\n",
      "Gradient Descent(46/99): loss=0.4109097927\n",
      "Gradient Descent(47/99): loss=0.4107316543\n",
      "Gradient Descent(48/99): loss=0.4105583125\n",
      "Gradient Descent(49/99): loss=0.4103895162\n",
      "Gradient Descent(50/99): loss=0.4102250316\n",
      "Gradient Descent(51/99): loss=0.4100646412\n",
      "Gradient Descent(52/99): loss=0.4099081425\n",
      "Gradient Descent(53/99): loss=0.4097553466\n",
      "Gradient Descent(54/99): loss=0.4096060774\n",
      "Gradient Descent(55/99): loss=0.4094601709\n",
      "Gradient Descent(56/99): loss=0.4093174736\n",
      "Gradient Descent(57/99): loss=0.4091778422\n",
      "Gradient Descent(58/99): loss=0.4090411429\n",
      "Gradient Descent(59/99): loss=0.4089072505\n",
      "Gradient Descent(60/99): loss=0.4087760476\n",
      "Gradient Descent(61/99): loss=0.4086474245\n",
      "Gradient Descent(62/99): loss=0.4085212781\n",
      "Gradient Descent(63/99): loss=0.4083975120\n",
      "Gradient Descent(64/99): loss=0.4082760354\n",
      "Gradient Descent(65/99): loss=0.4081567631\n",
      "Gradient Descent(66/99): loss=0.4080396149\n",
      "Gradient Descent(67/99): loss=0.4079245153\n",
      "Gradient Descent(68/99): loss=0.4078113934\n",
      "Gradient Descent(69/99): loss=0.4077001820\n",
      "Gradient Descent(70/99): loss=0.4075908180\n",
      "Gradient Descent(71/99): loss=0.4074832415\n",
      "Gradient Descent(72/99): loss=0.4073773961\n",
      "Gradient Descent(73/99): loss=0.4072732282\n",
      "Gradient Descent(74/99): loss=0.4071706872\n",
      "Gradient Descent(75/99): loss=0.4070697251\n",
      "Gradient Descent(76/99): loss=0.4069702962\n",
      "Gradient Descent(77/99): loss=0.4068723571\n",
      "Gradient Descent(78/99): loss=0.4067758667\n",
      "Gradient Descent(79/99): loss=0.4066807857\n",
      "Gradient Descent(80/99): loss=0.4065870766\n",
      "Gradient Descent(81/99): loss=0.4064947038\n",
      "Gradient Descent(82/99): loss=0.4064036332\n",
      "Gradient Descent(83/99): loss=0.4063138323\n",
      "Gradient Descent(84/99): loss=0.4062252697\n",
      "Gradient Descent(85/99): loss=0.4061379157\n",
      "Gradient Descent(86/99): loss=0.4060517416\n",
      "Gradient Descent(87/99): loss=0.4059667199\n",
      "Gradient Descent(88/99): loss=0.4058828243\n",
      "Gradient Descent(89/99): loss=0.4058000294\n",
      "Gradient Descent(90/99): loss=0.4057183107\n",
      "Gradient Descent(91/99): loss=0.4056376447\n",
      "Gradient Descent(92/99): loss=0.4055580088\n",
      "Gradient Descent(93/99): loss=0.4054793810\n",
      "Gradient Descent(94/99): loss=0.4054017403\n",
      "Gradient Descent(95/99): loss=0.4053250662\n",
      "Gradient Descent(96/99): loss=0.4052493390\n",
      "Gradient Descent(97/99): loss=0.4051745395\n",
      "Gradient Descent(98/99): loss=0.4051006493\n",
      "Gradient Descent(99/99): loss=0.4050276503\n"
     ]
    }
   ],
   "source": [
    "w_ls_sg, loss_ls_gd = least_squares_GD(y, tx, initial_w, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "453e177a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.2035595 , -0.12962203, -0.05527349,  0.01229099, -0.02450509,\n",
       "        0.19385846, -0.02780161, -0.02976163, -0.0456469 , -0.03147113,\n",
       "       -0.03162885, -0.02774737, -0.02520986,  0.00993751, -0.02963122,\n",
       "       -0.02967984, -0.05291429, -0.02962913, -0.02952568, -0.0311228 ,\n",
       "       -0.02949501, -0.05152506, -0.03020389,  0.02824165,  0.01292121,\n",
       "        0.01291545, -0.03789906, -0.02545391, -0.02550089, -0.04770133])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_ls_sg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e2203f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/99): loss=1.0000000000\n",
      "Gradient Descent(1/99): loss=1.3057120178\n",
      "Gradient Descent(2/99): loss=1.0654846161\n",
      "Gradient Descent(3/99): loss=2.4556311668\n",
      "Gradient Descent(4/99): loss=3.5352898015\n",
      "Gradient Descent(5/99): loss=2.6580898394\n",
      "Gradient Descent(6/99): loss=1.4466263320\n",
      "Gradient Descent(7/99): loss=1.9062438208\n",
      "Gradient Descent(8/99): loss=1.6942988594\n",
      "Gradient Descent(9/99): loss=1.6944467485\n",
      "Gradient Descent(10/99): loss=2.8010595636\n",
      "Gradient Descent(11/99): loss=1.3413237539\n",
      "Gradient Descent(12/99): loss=2.0673461964\n",
      "Gradient Descent(13/99): loss=1.3476098843\n",
      "Gradient Descent(14/99): loss=2.1680015315\n",
      "Gradient Descent(15/99): loss=2.7306333926\n",
      "Gradient Descent(16/99): loss=1.9052945848\n",
      "Gradient Descent(17/99): loss=1.9181537753\n",
      "Gradient Descent(18/99): loss=1.4767661456\n",
      "Gradient Descent(19/99): loss=2.1310986616\n",
      "Gradient Descent(20/99): loss=3.8156786178\n",
      "Gradient Descent(21/99): loss=2.2539738750\n",
      "Gradient Descent(22/99): loss=1.7614698252\n",
      "Gradient Descent(23/99): loss=3.3712113785\n",
      "Gradient Descent(24/99): loss=3.5371646327\n",
      "Gradient Descent(25/99): loss=3.4464975964\n",
      "Gradient Descent(26/99): loss=4.3341641561\n",
      "Gradient Descent(27/99): loss=4.2853518924\n",
      "Gradient Descent(28/99): loss=3.8522811166\n",
      "Gradient Descent(29/99): loss=2.1803290490\n",
      "Gradient Descent(30/99): loss=2.9510599728\n",
      "Gradient Descent(31/99): loss=4.1379116260\n",
      "Gradient Descent(32/99): loss=2.1995799708\n",
      "Gradient Descent(33/99): loss=4.3074194988\n",
      "Gradient Descent(34/99): loss=2.2176286786\n",
      "Gradient Descent(35/99): loss=2.9808096439\n",
      "Gradient Descent(36/99): loss=4.9747111641\n",
      "Gradient Descent(37/99): loss=2.2175052073\n",
      "Gradient Descent(38/99): loss=2.2524074690\n",
      "Gradient Descent(39/99): loss=3.2748465321\n",
      "Gradient Descent(40/99): loss=2.3078403408\n",
      "Gradient Descent(41/99): loss=2.1140663291\n",
      "Gradient Descent(42/99): loss=1.4779516265\n",
      "Gradient Descent(43/99): loss=2.7324999987\n",
      "Gradient Descent(44/99): loss=4.3149680623\n",
      "Gradient Descent(45/99): loss=2.5290785512\n",
      "Gradient Descent(46/99): loss=2.5388769575\n",
      "Gradient Descent(47/99): loss=4.9137792030\n",
      "Gradient Descent(48/99): loss=5.4442751503\n",
      "Gradient Descent(49/99): loss=4.2251664861\n",
      "Gradient Descent(50/99): loss=4.7434414133\n",
      "Gradient Descent(51/99): loss=4.0802684073\n",
      "Gradient Descent(52/99): loss=4.6237259088\n",
      "Gradient Descent(53/99): loss=4.0716355253\n",
      "Gradient Descent(54/99): loss=3.9788852349\n",
      "Gradient Descent(55/99): loss=1.2237607073\n",
      "Gradient Descent(56/99): loss=1.2726479680\n",
      "Gradient Descent(57/99): loss=1.2990891379\n",
      "Gradient Descent(58/99): loss=1.1201556632\n",
      "Gradient Descent(59/99): loss=2.2071440318\n",
      "Gradient Descent(60/99): loss=2.8487297937\n",
      "Gradient Descent(61/99): loss=1.8720947587\n",
      "Gradient Descent(62/99): loss=3.6853597846\n",
      "Gradient Descent(63/99): loss=4.1905405381\n",
      "Gradient Descent(64/99): loss=1.9816075017\n",
      "Gradient Descent(65/99): loss=2.1261974225\n",
      "Gradient Descent(66/99): loss=4.6794821003\n",
      "Gradient Descent(67/99): loss=1.4937880185\n",
      "Gradient Descent(68/99): loss=2.5883901917\n",
      "Gradient Descent(69/99): loss=1.2766309514\n",
      "Gradient Descent(70/99): loss=1.4341627769\n",
      "Gradient Descent(71/99): loss=1.4110185470\n",
      "Gradient Descent(72/99): loss=1.4240223119\n",
      "Gradient Descent(73/99): loss=3.0364021713\n",
      "Gradient Descent(74/99): loss=1.7065659921\n",
      "Gradient Descent(75/99): loss=1.6567169931\n",
      "Gradient Descent(76/99): loss=1.5717899930\n",
      "Gradient Descent(77/99): loss=2.0320067713\n",
      "Gradient Descent(78/99): loss=1.7205335778\n",
      "Gradient Descent(79/99): loss=3.2574822008\n",
      "Gradient Descent(80/99): loss=3.2905551090\n",
      "Gradient Descent(81/99): loss=3.0843053605\n",
      "Gradient Descent(82/99): loss=1.8422768416\n",
      "Gradient Descent(83/99): loss=1.0205430332\n",
      "Gradient Descent(84/99): loss=1.0133559467\n",
      "Gradient Descent(85/99): loss=2.3759699232\n",
      "Gradient Descent(86/99): loss=2.2674827938\n",
      "Gradient Descent(87/99): loss=4.1439646361\n",
      "Gradient Descent(88/99): loss=3.0486054606\n",
      "Gradient Descent(89/99): loss=1.6742521824\n",
      "Gradient Descent(90/99): loss=2.4093983067\n",
      "Gradient Descent(91/99): loss=1.7925315076\n",
      "Gradient Descent(92/99): loss=3.3226156419\n",
      "Gradient Descent(93/99): loss=3.3522028729\n",
      "Gradient Descent(94/99): loss=2.1814568873\n",
      "Gradient Descent(95/99): loss=2.1811543636\n",
      "Gradient Descent(96/99): loss=2.3054579099\n",
      "Gradient Descent(97/99): loss=2.1016279693\n",
      "Gradient Descent(98/99): loss=1.4522721015\n",
      "Gradient Descent(99/99): loss=2.0960511799\n"
     ]
    }
   ],
   "source": [
    "w_ls_sgd, loss_ls_sgd = least_squares_SGD(y, tx, initial_w, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3eabaf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_ls, loss_ls = least_squares(y, tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1386e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_rr, loss_rr = ridge_regression(y, tx, lambda_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ecc2d8d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/99): loss=173286.7951399862\n",
      "Gradient Descent(1/99): loss=167371.2566432319\n",
      "Gradient Descent(2/99): loss=163280.4137884185\n",
      "Gradient Descent(3/99): loss=160612.3812308610\n",
      "Gradient Descent(4/99): loss=160771.2364011554\n",
      "Gradient Descent(5/99): loss=160476.0834131945\n",
      "Gradient Descent(6/99): loss=158730.1683917697\n",
      "Gradient Descent(7/99): loss=157753.1319766692\n",
      "Gradient Descent(8/99): loss=160393.5211294697\n",
      "Gradient Descent(9/99): loss=160487.2845154690\n",
      "Gradient Descent(10/99): loss=160678.0608520149\n",
      "Gradient Descent(11/99): loss=164032.4072089480\n",
      "Gradient Descent(12/99): loss=160879.1782821260\n",
      "Gradient Descent(13/99): loss=164339.1654566197\n",
      "Gradient Descent(14/99): loss=161443.4559832202\n",
      "Gradient Descent(15/99): loss=159483.7740409501\n",
      "Gradient Descent(16/99): loss=159780.6176535089\n",
      "Gradient Descent(17/99): loss=158461.4826320697\n",
      "Gradient Descent(18/99): loss=157717.8254211573\n",
      "Gradient Descent(19/99): loss=157836.4541002009\n",
      "Gradient Descent(20/99): loss=157504.0120086611\n",
      "Gradient Descent(21/99): loss=157549.8648881899\n",
      "Gradient Descent(22/99): loss=158653.3343891683\n",
      "Gradient Descent(23/99): loss=161861.1157484503\n",
      "Gradient Descent(24/99): loss=161251.8516131045\n",
      "Gradient Descent(25/99): loss=159165.8255649122\n",
      "Gradient Descent(26/99): loss=158005.1353041441\n",
      "Gradient Descent(27/99): loss=157507.6256720582\n",
      "Gradient Descent(28/99): loss=157337.6175109716\n",
      "Gradient Descent(29/99): loss=157750.5241755476\n",
      "Gradient Descent(30/99): loss=158190.0169173131\n",
      "Gradient Descent(31/99): loss=157489.9728603858\n",
      "Gradient Descent(32/99): loss=157465.5367101849\n",
      "Gradient Descent(33/99): loss=157475.9818939383\n",
      "Gradient Descent(34/99): loss=157374.6177238351\n",
      "Gradient Descent(35/99): loss=157469.9136927509\n",
      "Gradient Descent(36/99): loss=156934.1265359902\n",
      "Gradient Descent(37/99): loss=157109.1539884351\n",
      "Gradient Descent(38/99): loss=157177.3645473276\n",
      "Gradient Descent(39/99): loss=157309.0655884666\n",
      "Gradient Descent(40/99): loss=157168.1589811136\n",
      "Gradient Descent(41/99): loss=157321.8598611621\n",
      "Gradient Descent(42/99): loss=157347.3461458444\n",
      "Gradient Descent(43/99): loss=157422.7280656314\n",
      "Gradient Descent(44/99): loss=156668.6286347377\n",
      "Gradient Descent(45/99): loss=156734.3648060091\n",
      "Gradient Descent(46/99): loss=157181.9692767586\n",
      "Gradient Descent(47/99): loss=157647.4194038707\n",
      "Gradient Descent(48/99): loss=158469.7077888047\n",
      "Gradient Descent(49/99): loss=157244.2751801585\n",
      "Gradient Descent(50/99): loss=157602.5575985849\n",
      "Gradient Descent(51/99): loss=157964.3481549305\n",
      "Gradient Descent(52/99): loss=158536.1022439527\n",
      "Gradient Descent(53/99): loss=157112.7905712281\n",
      "Gradient Descent(54/99): loss=157352.9313505347\n",
      "Gradient Descent(55/99): loss=157926.3447942980\n",
      "Gradient Descent(56/99): loss=158167.3690967174\n",
      "Gradient Descent(57/99): loss=158817.2276492112\n",
      "Gradient Descent(58/99): loss=158629.9627173730\n",
      "Gradient Descent(59/99): loss=159591.4542285926\n",
      "Gradient Descent(60/99): loss=157205.6035707255\n",
      "Gradient Descent(61/99): loss=157556.5730460082\n",
      "Gradient Descent(62/99): loss=158320.1457092038\n",
      "Gradient Descent(63/99): loss=159038.6588996947\n",
      "Gradient Descent(64/99): loss=159857.4350726324\n",
      "Gradient Descent(65/99): loss=160728.2412901968\n",
      "Gradient Descent(66/99): loss=161895.1689235083\n",
      "Gradient Descent(67/99): loss=161636.1390040219\n",
      "Gradient Descent(68/99): loss=158077.2362609257\n",
      "Gradient Descent(69/99): loss=156264.0114511049\n",
      "Gradient Descent(70/99): loss=156820.8839250655\n",
      "Gradient Descent(71/99): loss=157546.1195121022\n",
      "Gradient Descent(72/99): loss=158461.8892840176\n",
      "Gradient Descent(73/99): loss=159478.7053583885\n",
      "Gradient Descent(74/99): loss=160617.3168313105\n",
      "Gradient Descent(75/99): loss=161550.4074342873\n",
      "Gradient Descent(76/99): loss=162837.1892881161\n",
      "Gradient Descent(77/99): loss=163188.2636116578\n",
      "Gradient Descent(78/99): loss=162842.4542905737\n",
      "Gradient Descent(79/99): loss=164151.5998604036\n",
      "Gradient Descent(80/99): loss=165499.8835120196\n",
      "Gradient Descent(81/99): loss=166788.4838017850\n",
      "Gradient Descent(82/99): loss=160729.7292986885\n",
      "Gradient Descent(83/99): loss=160941.0025739901\n",
      "Gradient Descent(84/99): loss=161887.3180120520\n",
      "Gradient Descent(85/99): loss=163134.8276382637\n",
      "Gradient Descent(86/99): loss=163456.0649676677\n",
      "Gradient Descent(87/99): loss=164038.5447069698\n",
      "Gradient Descent(88/99): loss=165052.2311366959\n",
      "Gradient Descent(89/99): loss=164601.5025833595\n",
      "Gradient Descent(90/99): loss=160007.9997235262\n",
      "Gradient Descent(91/99): loss=161209.3508302390\n",
      "Gradient Descent(92/99): loss=162186.1366461169\n",
      "Gradient Descent(93/99): loss=161774.8058513085\n",
      "Gradient Descent(94/99): loss=158034.5589114605\n",
      "Gradient Descent(95/99): loss=158785.0578916279\n",
      "Gradient Descent(96/99): loss=159941.0790915957\n",
      "Gradient Descent(97/99): loss=161113.5616411735\n",
      "Gradient Descent(98/99): loss=161451.2446253664\n",
      "Gradient Descent(99/99): loss=162241.8761312773\n"
     ]
    }
   ],
   "source": [
    "w_lr, loss_lr = logistic_regression(y, tx, initial_w, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "437f3a4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/99): loss=173286.7951399862\n",
      "Gradient Descent(1/99): loss=178776.9669063461\n",
      "Gradient Descent(2/99): loss=172877.0018529257\n",
      "Gradient Descent(3/99): loss=168326.3077416788\n",
      "Gradient Descent(4/99): loss=169184.2314204616\n",
      "Gradient Descent(5/99): loss=164687.7576053823\n",
      "Gradient Descent(6/99): loss=161443.2054102583\n",
      "Gradient Descent(7/99): loss=159547.5273408344\n",
      "Gradient Descent(8/99): loss=159705.6477917326\n",
      "Gradient Descent(9/99): loss=162416.3108747509\n",
      "Gradient Descent(10/99): loss=162753.2931191724\n",
      "Gradient Descent(11/99): loss=160756.2211175978\n",
      "Gradient Descent(12/99): loss=158906.9696986105\n",
      "Gradient Descent(13/99): loss=157825.4655308444\n",
      "Gradient Descent(14/99): loss=157719.7303032613\n",
      "Gradient Descent(15/99): loss=157535.1685713945\n",
      "Gradient Descent(16/99): loss=157594.0300478773\n",
      "Gradient Descent(17/99): loss=157700.6334137765\n",
      "Gradient Descent(18/99): loss=157854.9037800505\n",
      "Gradient Descent(19/99): loss=157638.8563327049\n",
      "Gradient Descent(20/99): loss=157773.8726524371\n",
      "Gradient Descent(21/99): loss=157942.4124237126\n",
      "Gradient Descent(22/99): loss=158157.6544597759\n",
      "Gradient Descent(23/99): loss=157649.4683621874\n",
      "Gradient Descent(24/99): loss=157866.9168148889\n",
      "Gradient Descent(25/99): loss=158629.8130124210\n",
      "Gradient Descent(26/99): loss=158962.1086929227\n",
      "Gradient Descent(27/99): loss=158308.0983638300\n",
      "Gradient Descent(28/99): loss=157873.6584371027\n",
      "Gradient Descent(29/99): loss=157985.3332594500\n",
      "Gradient Descent(30/99): loss=158195.6048002700\n",
      "Gradient Descent(31/99): loss=158533.5054236451\n",
      "Gradient Descent(32/99): loss=160356.2725169722\n",
      "Gradient Descent(33/99): loss=159107.0095996888\n",
      "Gradient Descent(34/99): loss=161434.0491128177\n",
      "Gradient Descent(35/99): loss=162190.4375701174\n",
      "Gradient Descent(36/99): loss=165758.0508337115\n",
      "Gradient Descent(37/99): loss=170371.3491949378\n",
      "Gradient Descent(38/99): loss=171094.4672904485\n",
      "Gradient Descent(39/99): loss=168952.4222505220\n",
      "Gradient Descent(40/99): loss=164507.1924086205\n",
      "Gradient Descent(41/99): loss=164984.7023491797\n",
      "Gradient Descent(42/99): loss=171146.9729538174\n",
      "Gradient Descent(43/99): loss=166760.2702677563\n",
      "Gradient Descent(44/99): loss=162746.5185456162\n",
      "Gradient Descent(45/99): loss=166589.6626224866\n",
      "Gradient Descent(46/99): loss=165529.6207346513\n",
      "Gradient Descent(47/99): loss=170246.1043011103\n",
      "Gradient Descent(48/99): loss=170764.0859232298\n",
      "Gradient Descent(49/99): loss=165810.6340195683\n",
      "Gradient Descent(50/99): loss=172241.0444571635\n",
      "Gradient Descent(51/99): loss=171348.8594490535\n",
      "Gradient Descent(52/99): loss=166684.9669329014\n",
      "Gradient Descent(53/99): loss=166178.9259313587\n",
      "Gradient Descent(54/99): loss=162879.0827620180\n",
      "Gradient Descent(55/99): loss=168541.6852269540\n",
      "Gradient Descent(56/99): loss=169602.9545107853\n",
      "Gradient Descent(57/99): loss=165712.7416692303\n",
      "Gradient Descent(58/99): loss=161969.4926483382\n",
      "Gradient Descent(59/99): loss=160049.9786927022\n",
      "Gradient Descent(60/99): loss=160521.1289418577\n",
      "Gradient Descent(61/99): loss=163771.2360415348\n",
      "Gradient Descent(62/99): loss=164252.2041197066\n",
      "Gradient Descent(63/99): loss=161782.3702287415\n",
      "Gradient Descent(64/99): loss=160112.9780733834\n",
      "Gradient Descent(65/99): loss=164350.9018714029\n",
      "Gradient Descent(66/99): loss=163565.9637400715\n",
      "Gradient Descent(67/99): loss=164052.5562332663\n",
      "Gradient Descent(68/99): loss=168286.4417451867\n",
      "Gradient Descent(69/99): loss=168937.7993259348\n",
      "Gradient Descent(70/99): loss=164181.8631628636\n",
      "Gradient Descent(71/99): loss=163034.2722927672\n",
      "Gradient Descent(72/99): loss=162513.3231047894\n",
      "Gradient Descent(73/99): loss=159854.4447611099\n",
      "Gradient Descent(74/99): loss=158044.4272436473\n",
      "Gradient Descent(75/99): loss=161742.4607146792\n",
      "Gradient Descent(76/99): loss=159709.8716619261\n",
      "Gradient Descent(77/99): loss=158045.3105473266\n",
      "Gradient Descent(78/99): loss=157875.6921279456\n",
      "Gradient Descent(79/99): loss=157490.7469392348\n",
      "Gradient Descent(80/99): loss=159832.3922671312\n",
      "Gradient Descent(81/99): loss=163220.4976499917\n",
      "Gradient Descent(82/99): loss=160318.8244039599\n",
      "Gradient Descent(83/99): loss=158393.3841609505\n",
      "Gradient Descent(84/99): loss=162480.1328559096\n",
      "Gradient Descent(85/99): loss=159718.2074498142\n",
      "Gradient Descent(86/99): loss=158096.7033942946\n",
      "Gradient Descent(87/99): loss=157143.2927303798\n",
      "Gradient Descent(88/99): loss=156628.9432826735\n",
      "Gradient Descent(89/99): loss=156708.8357704099\n",
      "Gradient Descent(90/99): loss=156521.3794369102\n",
      "Gradient Descent(91/99): loss=156558.6696845139\n",
      "Gradient Descent(92/99): loss=156633.7519249982\n",
      "Gradient Descent(93/99): loss=156497.8842084540\n",
      "Gradient Descent(94/99): loss=156871.9163722846\n",
      "Gradient Descent(95/99): loss=157369.7912968376\n",
      "Gradient Descent(96/99): loss=158108.9814021880\n",
      "Gradient Descent(97/99): loss=157988.4098918111\n",
      "Gradient Descent(98/99): loss=157938.3080692704\n",
      "Gradient Descent(99/99): loss=157563.8528583255\n"
     ]
    }
   ],
   "source": [
    "w_rlr, loss_rlr = reg_logistic_regression(y, tx, lambda_, initial_w, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5dddef8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.69238\n"
     ]
    }
   ],
   "source": [
    "train_X = tx[:200000]\n",
    "train_y = y[:200000]\n",
    "\n",
    "test_X = tx[200000:]\n",
    "test_y = y[200000:]\n",
    "\n",
    "#w_lr, loss_lr = logistic_regression(train_y, train_X, initial_w, max_iters, gamma)\n",
    "\n",
    "#preds = 1 / (1 + np.exp(-(test_X @ w_lr)))\n",
    "#preds[preds <= 0.5] = -1\n",
    "#preds[preds > 0.5] = 1\n",
    "\n",
    "w_lr, loss_rr = ridge_regression(train_y, train_X, lambda_)\n",
    "\n",
    "preds = test_X @ w_lr\n",
    "preds[preds <= 0] = -1\n",
    "preds[preds > 0] = 1\n",
    "\n",
    "print(np.mean((preds == test_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970b4c53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
