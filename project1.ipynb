{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from proj1_helpers import *\n",
    "from implementations import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data/train.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, x, ids = load_csv_data(data_path, sub_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx, mean_x, std_x = standardize(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SGD parameters\n",
    "initial_w = np.zeros((x.shape[1]))\n",
    "max_iters = 100\n",
    "gamma = 0.05\n",
    "lambda_ = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/99): loss=0.5000000000\n",
      "Gradient Descent(1/99): loss=0.4401420042\n",
      "Gradient Descent(2/99): loss=0.4367786984\n",
      "Gradient Descent(3/99): loss=0.4349683956\n",
      "Gradient Descent(4/99): loss=0.4333423494\n",
      "Gradient Descent(5/99): loss=0.4318501655\n",
      "Gradient Descent(6/99): loss=0.4304762522\n",
      "Gradient Descent(7/99): loss=0.4292078895\n",
      "Gradient Descent(8/99): loss=0.4280341066\n",
      "Gradient Descent(9/99): loss=0.4269453800\n",
      "Gradient Descent(10/99): loss=0.4259334015\n",
      "Gradient Descent(11/99): loss=0.4249908885\n",
      "Gradient Descent(12/99): loss=0.4241114286\n",
      "Gradient Descent(13/99): loss=0.4232893508\n",
      "Gradient Descent(14/99): loss=0.4225196188\n",
      "Gradient Descent(15/99): loss=0.4217977432\n",
      "Gradient Descent(16/99): loss=0.4211197069\n",
      "Gradient Descent(17/99): loss=0.4204819036\n",
      "Gradient Descent(18/99): loss=0.4198810859\n",
      "Gradient Descent(19/99): loss=0.4193143206\n",
      "Gradient Descent(20/99): loss=0.4187789520\n",
      "Gradient Descent(21/99): loss=0.4182725695\n",
      "Gradient Descent(22/99): loss=0.4177929803\n",
      "Gradient Descent(23/99): loss=0.4173381857\n",
      "Gradient Descent(24/99): loss=0.4169063611\n",
      "Gradient Descent(25/99): loss=0.4164958376\n",
      "Gradient Descent(26/99): loss=0.4161050868\n",
      "Gradient Descent(27/99): loss=0.4157327071\n",
      "Gradient Descent(28/99): loss=0.4153774116\n",
      "Gradient Descent(29/99): loss=0.4150380176\n",
      "Gradient Descent(30/99): loss=0.4147134369\n",
      "Gradient Descent(31/99): loss=0.4144026675\n",
      "Gradient Descent(32/99): loss=0.4141047860\n",
      "Gradient Descent(33/99): loss=0.4138189408\n",
      "Gradient Descent(34/99): loss=0.4135443460\n",
      "Gradient Descent(35/99): loss=0.4132802759\n",
      "Gradient Descent(36/99): loss=0.4130260597\n",
      "Gradient Descent(37/99): loss=0.4127810777\n",
      "Gradient Descent(38/99): loss=0.4125447561\n",
      "Gradient Descent(39/99): loss=0.4123165644\n",
      "Gradient Descent(40/99): loss=0.4120960110\n",
      "Gradient Descent(41/99): loss=0.4118826408\n",
      "Gradient Descent(42/99): loss=0.4116760316\n",
      "Gradient Descent(43/99): loss=0.4114757924\n",
      "Gradient Descent(44/99): loss=0.4112815600\n",
      "Gradient Descent(45/99): loss=0.4110929977\n",
      "Gradient Descent(46/99): loss=0.4109097927\n",
      "Gradient Descent(47/99): loss=0.4107316543\n",
      "Gradient Descent(48/99): loss=0.4105583125\n",
      "Gradient Descent(49/99): loss=0.4103895162\n",
      "Gradient Descent(50/99): loss=0.4102250316\n",
      "Gradient Descent(51/99): loss=0.4100646412\n",
      "Gradient Descent(52/99): loss=0.4099081425\n",
      "Gradient Descent(53/99): loss=0.4097553466\n",
      "Gradient Descent(54/99): loss=0.4096060774\n",
      "Gradient Descent(55/99): loss=0.4094601709\n",
      "Gradient Descent(56/99): loss=0.4093174736\n",
      "Gradient Descent(57/99): loss=0.4091778422\n",
      "Gradient Descent(58/99): loss=0.4090411429\n",
      "Gradient Descent(59/99): loss=0.4089072505\n",
      "Gradient Descent(60/99): loss=0.4087760476\n",
      "Gradient Descent(61/99): loss=0.4086474245\n",
      "Gradient Descent(62/99): loss=0.4085212781\n",
      "Gradient Descent(63/99): loss=0.4083975120\n",
      "Gradient Descent(64/99): loss=0.4082760354\n",
      "Gradient Descent(65/99): loss=0.4081567631\n",
      "Gradient Descent(66/99): loss=0.4080396149\n",
      "Gradient Descent(67/99): loss=0.4079245153\n",
      "Gradient Descent(68/99): loss=0.4078113934\n",
      "Gradient Descent(69/99): loss=0.4077001820\n",
      "Gradient Descent(70/99): loss=0.4075908180\n",
      "Gradient Descent(71/99): loss=0.4074832415\n",
      "Gradient Descent(72/99): loss=0.4073773961\n",
      "Gradient Descent(73/99): loss=0.4072732282\n",
      "Gradient Descent(74/99): loss=0.4071706872\n",
      "Gradient Descent(75/99): loss=0.4070697251\n",
      "Gradient Descent(76/99): loss=0.4069702962\n",
      "Gradient Descent(77/99): loss=0.4068723571\n",
      "Gradient Descent(78/99): loss=0.4067758667\n",
      "Gradient Descent(79/99): loss=0.4066807857\n",
      "Gradient Descent(80/99): loss=0.4065870766\n",
      "Gradient Descent(81/99): loss=0.4064947038\n",
      "Gradient Descent(82/99): loss=0.4064036332\n",
      "Gradient Descent(83/99): loss=0.4063138323\n",
      "Gradient Descent(84/99): loss=0.4062252697\n",
      "Gradient Descent(85/99): loss=0.4061379157\n",
      "Gradient Descent(86/99): loss=0.4060517416\n",
      "Gradient Descent(87/99): loss=0.4059667199\n",
      "Gradient Descent(88/99): loss=0.4058828243\n",
      "Gradient Descent(89/99): loss=0.4058000294\n",
      "Gradient Descent(90/99): loss=0.4057183107\n",
      "Gradient Descent(91/99): loss=0.4056376447\n",
      "Gradient Descent(92/99): loss=0.4055580088\n",
      "Gradient Descent(93/99): loss=0.4054793810\n",
      "Gradient Descent(94/99): loss=0.4054017403\n",
      "Gradient Descent(95/99): loss=0.4053250662\n",
      "Gradient Descent(96/99): loss=0.4052493390\n",
      "Gradient Descent(97/99): loss=0.4051745395\n",
      "Gradient Descent(98/99): loss=0.4051006493\n",
      "Gradient Descent(99/99): loss=0.4050276503\n"
     ]
    }
   ],
   "source": [
    "w_ls_sg, loss_ls_gd = least_squares_GD(y, tx, initial_w, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.2035595 , -0.12962203, -0.05527349,  0.01229099, -0.02450509,\n",
       "        0.19385846, -0.02780161, -0.02976163, -0.0456469 , -0.03147113,\n",
       "       -0.03162885, -0.02774737, -0.02520986,  0.00993751, -0.02963122,\n",
       "       -0.02967984, -0.05291429, -0.02962913, -0.02952568, -0.0311228 ,\n",
       "       -0.02949501, -0.05152506, -0.03020389,  0.02824165,  0.01292121,\n",
       "        0.01291545, -0.03789906, -0.02545391, -0.02550089, -0.04770133])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_ls_sg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/99): loss=1.0000000000\n",
      "Gradient Descent(1/99): loss=1.4813874638\n",
      "Gradient Descent(2/99): loss=1.0722833481\n",
      "Gradient Descent(3/99): loss=1.4554742783\n",
      "Gradient Descent(4/99): loss=1.3517006424\n",
      "Gradient Descent(5/99): loss=0.9756417578\n",
      "Gradient Descent(6/99): loss=1.0267296891\n",
      "Gradient Descent(7/99): loss=2.2747130593\n",
      "Gradient Descent(8/99): loss=3.0607461057\n",
      "Gradient Descent(9/99): loss=1.3578937014\n",
      "Gradient Descent(10/99): loss=1.5018543906\n",
      "Gradient Descent(11/99): loss=1.9316182092\n",
      "Gradient Descent(12/99): loss=1.1732469939\n",
      "Gradient Descent(13/99): loss=2.7749717522\n",
      "Gradient Descent(14/99): loss=1.4532080740\n",
      "Gradient Descent(15/99): loss=1.3499270788\n",
      "Gradient Descent(16/99): loss=2.9026133833\n",
      "Gradient Descent(17/99): loss=1.1507894390\n",
      "Gradient Descent(18/99): loss=1.6062276462\n",
      "Gradient Descent(19/99): loss=1.4850328602\n",
      "Gradient Descent(20/99): loss=1.4171033264\n",
      "Gradient Descent(21/99): loss=1.7207348280\n",
      "Gradient Descent(22/99): loss=3.9661711871\n",
      "Gradient Descent(23/99): loss=5.8365993534\n",
      "Gradient Descent(24/99): loss=6.9707876004\n",
      "Gradient Descent(25/99): loss=5.7966524252\n",
      "Gradient Descent(26/99): loss=5.6668917970\n",
      "Gradient Descent(27/99): loss=9.2024653414\n",
      "Gradient Descent(28/99): loss=8.5543351202\n",
      "Gradient Descent(29/99): loss=6.8656181955\n",
      "Gradient Descent(30/99): loss=6.1175306951\n",
      "Gradient Descent(31/99): loss=2.5546835976\n",
      "Gradient Descent(32/99): loss=2.3558144494\n",
      "Gradient Descent(33/99): loss=2.2299614360\n",
      "Gradient Descent(34/99): loss=4.8850962335\n",
      "Gradient Descent(35/99): loss=2.2504834359\n",
      "Gradient Descent(36/99): loss=3.4147765594\n",
      "Gradient Descent(37/99): loss=6.0424512656\n",
      "Gradient Descent(38/99): loss=3.6852785507\n",
      "Gradient Descent(39/99): loss=6.2963643471\n",
      "Gradient Descent(40/99): loss=2.4514445559\n",
      "Gradient Descent(41/99): loss=2.9914951292\n",
      "Gradient Descent(42/99): loss=2.1080512361\n",
      "Gradient Descent(43/99): loss=4.6043833876\n",
      "Gradient Descent(44/99): loss=2.6163734258\n",
      "Gradient Descent(45/99): loss=3.2357376450\n",
      "Gradient Descent(46/99): loss=3.1826305494\n",
      "Gradient Descent(47/99): loss=5.4218737774\n",
      "Gradient Descent(48/99): loss=3.4101726141\n",
      "Gradient Descent(49/99): loss=3.1010078142\n",
      "Gradient Descent(50/99): loss=3.9276699395\n",
      "Gradient Descent(51/99): loss=3.7481369442\n",
      "Gradient Descent(52/99): loss=1.5046373438\n",
      "Gradient Descent(53/99): loss=1.5852325398\n",
      "Gradient Descent(54/99): loss=1.5505210798\n",
      "Gradient Descent(55/99): loss=1.5908660909\n",
      "Gradient Descent(56/99): loss=1.4564925511\n",
      "Gradient Descent(57/99): loss=2.8619205020\n",
      "Gradient Descent(58/99): loss=1.5259380721\n",
      "Gradient Descent(59/99): loss=1.3337713175\n",
      "Gradient Descent(60/99): loss=1.2987830427\n",
      "Gradient Descent(61/99): loss=1.4095072247\n",
      "Gradient Descent(62/99): loss=1.1749696284\n",
      "Gradient Descent(63/99): loss=1.4208495767\n",
      "Gradient Descent(64/99): loss=1.5697750589\n",
      "Gradient Descent(65/99): loss=1.2120327174\n",
      "Gradient Descent(66/99): loss=1.7574946010\n",
      "Gradient Descent(67/99): loss=2.1855858515\n",
      "Gradient Descent(68/99): loss=1.5932980467\n",
      "Gradient Descent(69/99): loss=2.5839436594\n",
      "Gradient Descent(70/99): loss=4.6612340214\n",
      "Gradient Descent(71/99): loss=2.9306154108\n",
      "Gradient Descent(72/99): loss=2.4107788385\n",
      "Gradient Descent(73/99): loss=4.1460666817\n",
      "Gradient Descent(74/99): loss=4.2687446604\n",
      "Gradient Descent(75/99): loss=5.5409638149\n",
      "Gradient Descent(76/99): loss=5.2172160741\n",
      "Gradient Descent(77/99): loss=4.5398395240\n",
      "Gradient Descent(78/99): loss=4.4932988359\n",
      "Gradient Descent(79/99): loss=5.5890233602\n",
      "Gradient Descent(80/99): loss=5.6906071809\n",
      "Gradient Descent(81/99): loss=2.8632245752\n",
      "Gradient Descent(82/99): loss=6.8613042181\n",
      "Gradient Descent(83/99): loss=6.8116580237\n",
      "Gradient Descent(84/99): loss=4.4688280808\n",
      "Gradient Descent(85/99): loss=4.6171792733\n",
      "Gradient Descent(86/99): loss=4.5486997469\n",
      "Gradient Descent(87/99): loss=4.6627976644\n",
      "Gradient Descent(88/99): loss=2.4991433541\n",
      "Gradient Descent(89/99): loss=1.5925412858\n",
      "Gradient Descent(90/99): loss=1.6708007548\n",
      "Gradient Descent(91/99): loss=1.5202798378\n",
      "Gradient Descent(92/99): loss=1.4768266687\n",
      "Gradient Descent(93/99): loss=1.5292187425\n",
      "Gradient Descent(94/99): loss=1.3870485421\n",
      "Gradient Descent(95/99): loss=3.0270178682\n",
      "Gradient Descent(96/99): loss=1.3980854845\n",
      "Gradient Descent(97/99): loss=1.3863143463\n",
      "Gradient Descent(98/99): loss=1.4194896067\n",
      "Gradient Descent(99/99): loss=3.4335831855\n"
     ]
    }
   ],
   "source": [
    "w_ls_sgd, loss_ls_sgd = least_squares_SGD(y, tx, initial_w, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_ls, loss_ls = least_squares(y, tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_rr, loss_rr = ridge_regression(y, tx, lambda_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/99): loss=173286.7951399862\n",
      "Gradient Descent(1/99): loss=210041.4048319565\n",
      "Gradient Descent(2/99): loss=165461.3983009645\n",
      "Gradient Descent(3/99): loss=164557.4039888914\n",
      "Gradient Descent(4/99): loss=159589.4073099275\n",
      "Gradient Descent(5/99): loss=163940.2916615861\n",
      "Gradient Descent(6/99): loss=170078.3005289238\n",
      "Gradient Descent(7/99): loss=177994.2839201881\n",
      "Gradient Descent(8/99): loss=185049.6577627024\n",
      "Gradient Descent(9/99): loss=191247.1412087666\n",
      "Gradient Descent(10/99): loss=196844.0026629237\n",
      "Gradient Descent(11/99): loss=201782.9031734972\n",
      "Gradient Descent(12/99): loss=197762.4754628168\n",
      "Gradient Descent(13/99): loss=200895.6972338420\n",
      "Gradient Descent(14/99): loss=161367.2350679715\n",
      "Gradient Descent(15/99): loss=164360.4695232406\n",
      "Gradient Descent(16/99): loss=170955.9414358234\n",
      "Gradient Descent(17/99): loss=174341.1306893851\n",
      "Gradient Descent(18/99): loss=179689.2490499566\n",
      "Gradient Descent(19/99): loss=165864.7077286528\n",
      "Gradient Descent(20/99): loss=205069.5780205900\n",
      "Gradient Descent(21/99): loss=234519.7827464648\n",
      "Gradient Descent(22/99): loss=259349.9118561799\n",
      "Gradient Descent(23/99): loss=297152.8796408013\n",
      "Gradient Descent(24/99): loss=314042.5835290874\n",
      "Gradient Descent(25/99): loss=328635.8368542625\n",
      "Gradient Descent(26/99): loss=341063.1178392780\n",
      "Gradient Descent(27/99): loss=222666.1460059737\n",
      "Gradient Descent(28/99): loss=224495.2454777991\n",
      "Gradient Descent(29/99): loss=223946.1859795518\n",
      "Gradient Descent(30/99): loss=168029.9405733761\n",
      "Gradient Descent(31/99): loss=192970.6395982922\n",
      "Gradient Descent(32/99): loss=168396.9375293359\n",
      "Gradient Descent(33/99): loss=213905.0301379347\n",
      "Gradient Descent(34/99): loss=238952.6302276799\n",
      "Gradient Descent(35/99): loss=261353.7783054771\n",
      "Gradient Descent(36/99): loss=264107.8808366475\n",
      "Gradient Descent(37/99): loss=282146.6698263006\n",
      "Gradient Descent(38/99): loss=190650.9179814646\n",
      "Gradient Descent(39/99): loss=168257.2383642580\n",
      "Gradient Descent(40/99): loss=165299.5044374445\n",
      "Gradient Descent(41/99): loss=162355.3258924262\n",
      "Gradient Descent(42/99): loss=159742.1447277288\n",
      "Gradient Descent(43/99): loss=157986.7766768072\n",
      "Gradient Descent(44/99): loss=158391.4349366255\n",
      "Gradient Descent(45/99): loss=198766.6600533529\n",
      "Gradient Descent(46/99): loss=161390.4022343409\n",
      "Gradient Descent(47/99): loss=158614.4401666736\n",
      "Gradient Descent(48/99): loss=168283.0169787439\n",
      "Gradient Descent(49/99): loss=173481.3746351457\n",
      "Gradient Descent(50/99): loss=177001.1367984178\n",
      "Gradient Descent(51/99): loss=173869.2516557717\n",
      "Gradient Descent(52/99): loss=183662.8084132930\n",
      "Gradient Descent(53/99): loss=192230.4484718482\n",
      "Gradient Descent(54/99): loss=195754.3944890086\n",
      "Gradient Descent(55/99): loss=153781.5228655410\n",
      "Gradient Descent(56/99): loss=158571.8803824482\n",
      "Gradient Descent(57/99): loss=164954.6154199108\n",
      "Gradient Descent(58/99): loss=156365.9674361888\n",
      "Gradient Descent(59/99): loss=156569.6966501407\n",
      "Gradient Descent(60/99): loss=161467.6019847576\n",
      "Gradient Descent(61/99): loss=155985.2611233854\n",
      "Gradient Descent(62/99): loss=162529.8671556999\n",
      "Gradient Descent(63/99): loss=159499.2129220541\n",
      "Gradient Descent(64/99): loss=159235.2036816058\n",
      "Gradient Descent(65/99): loss=157033.3907210445\n",
      "Gradient Descent(66/99): loss=156841.5485037526\n",
      "Gradient Descent(67/99): loss=163460.3646283229\n",
      "Gradient Descent(68/99): loss=163256.3463623857\n",
      "Gradient Descent(69/99): loss=164008.3848305744\n",
      "Gradient Descent(70/99): loss=175252.2694664194\n",
      "Gradient Descent(71/99): loss=178776.8433619529\n",
      "Gradient Descent(72/99): loss=167092.0106356449\n",
      "Gradient Descent(73/99): loss=195602.2654014428\n",
      "Gradient Descent(74/99): loss=161724.1173972017\n",
      "Gradient Descent(75/99): loss=155605.4415011629\n",
      "Gradient Descent(76/99): loss=163296.5518273825\n",
      "Gradient Descent(77/99): loss=163865.8826860410\n",
      "Gradient Descent(78/99): loss=169745.5549066630\n",
      "Gradient Descent(79/99): loss=164852.5602842909\n",
      "Gradient Descent(80/99): loss=158384.8637775131\n",
      "Gradient Descent(81/99): loss=152319.5986443954\n",
      "Gradient Descent(82/99): loss=154504.6768862101\n",
      "Gradient Descent(83/99): loss=153934.9645177050\n",
      "Gradient Descent(84/99): loss=154784.5466000707\n",
      "Gradient Descent(85/99): loss=172216.7053505889\n",
      "Gradient Descent(86/99): loss=151919.9322481256\n",
      "Gradient Descent(87/99): loss=153203.0399681171\n",
      "Gradient Descent(88/99): loss=157165.0697920988\n",
      "Gradient Descent(89/99): loss=163936.8359598144\n",
      "Gradient Descent(90/99): loss=168752.6244711226\n",
      "Gradient Descent(91/99): loss=165887.2811385303\n",
      "Gradient Descent(92/99): loss=169114.5744948764\n",
      "Gradient Descent(93/99): loss=177884.8997787529\n",
      "Gradient Descent(94/99): loss=152334.0528049098\n",
      "Gradient Descent(95/99): loss=151510.5704320278\n",
      "Gradient Descent(96/99): loss=183932.9788209165\n",
      "Gradient Descent(97/99): loss=151270.4911062839\n",
      "Gradient Descent(98/99): loss=152257.4532838528\n",
      "Gradient Descent(99/99): loss=159303.3999646385\n"
     ]
    }
   ],
   "source": [
    "w_lr, loss_lr = logistic_regression(y, tx, initial_w, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000,)\n",
      "Gradient Descent(0/99): loss=173286.7951399862\n",
      "(250000,)\n",
      "Gradient Descent(1/99): loss=178416.4760899910\n",
      "(250000,)\n",
      "Gradient Descent(2/99): loss=216714.0635692364\n",
      "(250000,)\n",
      "Gradient Descent(3/99): loss=199987.0656841476\n",
      "(250000,)\n",
      "Gradient Descent(4/99): loss=162686.0102535001\n",
      "(250000,)\n",
      "Gradient Descent(5/99): loss=159180.0027656191\n",
      "(250000,)\n",
      "Gradient Descent(6/99): loss=164512.7720283768\n",
      "(250000,)\n",
      "Gradient Descent(7/99): loss=166609.6582855356\n",
      "(250000,)\n",
      "Gradient Descent(8/99): loss=175673.6127363278\n",
      "(250000,)\n",
      "Gradient Descent(9/99): loss=215442.0540682789\n",
      "(250000,)\n",
      "Gradient Descent(10/99): loss=161264.2552360438\n",
      "(250000,)\n",
      "Gradient Descent(11/99): loss=189724.8461971339\n",
      "(250000,)\n",
      "Gradient Descent(12/99): loss=238491.6943682208\n",
      "(250000,)\n",
      "Gradient Descent(13/99): loss=168798.8003478295\n",
      "(250000,)\n",
      "Gradient Descent(14/99): loss=158854.8290709870\n",
      "(250000,)\n",
      "Gradient Descent(15/99): loss=160231.7342841981\n",
      "(250000,)\n",
      "Gradient Descent(16/99): loss=164708.2245497691\n",
      "(250000,)\n",
      "Gradient Descent(17/99): loss=165156.2860275690\n",
      "(250000,)\n",
      "Gradient Descent(18/99): loss=160802.0899684720\n",
      "(250000,)\n",
      "Gradient Descent(19/99): loss=168413.7528454217\n",
      "(250000,)\n",
      "Gradient Descent(20/99): loss=162917.2517922955\n",
      "(250000,)\n",
      "Gradient Descent(21/99): loss=167729.0408192555\n",
      "(250000,)\n",
      "Gradient Descent(22/99): loss=169270.1208570289\n",
      "(250000,)\n",
      "Gradient Descent(23/99): loss=176795.1983151989\n",
      "(250000,)\n",
      "Gradient Descent(24/99): loss=157971.0158413712\n",
      "(250000,)\n",
      "Gradient Descent(25/99): loss=159828.4737784031\n",
      "(250000,)\n",
      "Gradient Descent(26/99): loss=197992.6952837041\n",
      "(250000,)\n",
      "Gradient Descent(27/99): loss=203147.1018940685\n",
      "(250000,)\n",
      "Gradient Descent(28/99): loss=231900.2284383175\n",
      "(250000,)\n",
      "Gradient Descent(29/99): loss=225955.8801062782\n",
      "(250000,)\n",
      "Gradient Descent(30/99): loss=165602.0332742042\n",
      "(250000,)\n",
      "Gradient Descent(31/99): loss=190267.4840181215\n",
      "(250000,)\n",
      "Gradient Descent(32/99): loss=190437.8974919549\n",
      "(250000,)\n",
      "Gradient Descent(33/99): loss=214108.5269326149\n",
      "(250000,)\n",
      "Gradient Descent(34/99): loss=210921.1990969090\n",
      "(250000,)\n",
      "Gradient Descent(35/99): loss=173972.1428495275\n",
      "(250000,)\n",
      "Gradient Descent(36/99): loss=168940.6425251575\n",
      "(250000,)\n",
      "Gradient Descent(37/99): loss=167572.0670436378\n",
      "(250000,)\n",
      "Gradient Descent(38/99): loss=170759.7505208457\n",
      "(250000,)\n",
      "Gradient Descent(39/99): loss=227214.3185450013\n",
      "(250000,)\n",
      "Gradient Descent(40/99): loss=165158.4495934453\n",
      "(250000,)\n",
      "Gradient Descent(41/99): loss=158715.8684634446\n",
      "(250000,)\n",
      "Gradient Descent(42/99): loss=157691.8940202713\n",
      "(250000,)\n",
      "Gradient Descent(43/99): loss=159124.5514746842\n",
      "(250000,)\n",
      "Gradient Descent(44/99): loss=159869.2525185779\n",
      "(250000,)\n",
      "Gradient Descent(45/99): loss=167855.9193241642\n",
      "(250000,)\n",
      "Gradient Descent(46/99): loss=164784.0609917459\n",
      "(250000,)\n",
      "Gradient Descent(47/99): loss=163524.9662349070\n",
      "(250000,)\n",
      "Gradient Descent(48/99): loss=167610.4972490725\n",
      "(250000,)\n",
      "Gradient Descent(49/99): loss=174175.4341597114\n",
      "(250000,)\n",
      "Gradient Descent(50/99): loss=160688.7736164546\n",
      "(250000,)\n",
      "Gradient Descent(51/99): loss=158856.2159566495\n",
      "(250000,)\n",
      "Gradient Descent(52/99): loss=161533.6554360825\n",
      "(250000,)\n",
      "Gradient Descent(53/99): loss=169215.1517950957\n",
      "(250000,)\n",
      "Gradient Descent(54/99): loss=167108.1022577730\n",
      "(250000,)\n",
      "Gradient Descent(55/99): loss=167618.1014067909\n",
      "(250000,)\n",
      "Gradient Descent(56/99): loss=161236.2748069522\n",
      "(250000,)\n",
      "Gradient Descent(57/99): loss=157220.7007824196\n",
      "(250000,)\n",
      "Gradient Descent(58/99): loss=177383.9624985764\n",
      "(250000,)\n",
      "Gradient Descent(59/99): loss=182434.1304920557\n",
      "(250000,)\n",
      "Gradient Descent(60/99): loss=209741.0835389833\n",
      "(250000,)\n",
      "Gradient Descent(61/99): loss=164601.3392104977\n",
      "(250000,)\n",
      "Gradient Descent(62/99): loss=218482.1562769878\n",
      "(250000,)\n",
      "Gradient Descent(63/99): loss=164166.2564049063\n",
      "(250000,)\n",
      "Gradient Descent(64/99): loss=213650.4581361088\n",
      "(250000,)\n",
      "Gradient Descent(65/99): loss=214882.2251422550\n",
      "(250000,)\n",
      "Gradient Descent(66/99): loss=161864.7382692109\n",
      "(250000,)\n",
      "Gradient Descent(67/99): loss=160326.3119201600\n",
      "(250000,)\n",
      "Gradient Descent(68/99): loss=207206.9452945974\n",
      "(250000,)\n",
      "Gradient Descent(69/99): loss=245915.0140234471\n",
      "(250000,)\n",
      "Gradient Descent(70/99): loss=173645.6406062197\n",
      "(250000,)\n",
      "Gradient Descent(71/99): loss=157587.4103565670\n",
      "(250000,)\n",
      "Gradient Descent(72/99): loss=207638.0846677769\n",
      "(250000,)\n",
      "Gradient Descent(73/99): loss=253710.5382524126\n",
      "(250000,)\n",
      "Gradient Descent(74/99): loss=185139.2351319683\n",
      "(250000,)\n",
      "Gradient Descent(75/99): loss=166726.4339176571\n",
      "(250000,)\n",
      "Gradient Descent(76/99): loss=203895.9502342786\n",
      "(250000,)\n",
      "Gradient Descent(77/99): loss=170113.5531945029\n",
      "(250000,)\n",
      "Gradient Descent(78/99): loss=163927.3018238977\n",
      "(250000,)\n",
      "Gradient Descent(79/99): loss=161497.6354959391\n",
      "(250000,)\n",
      "Gradient Descent(80/99): loss=194555.2822132314\n",
      "(250000,)\n",
      "Gradient Descent(81/99): loss=171860.4327375997\n",
      "(250000,)\n",
      "Gradient Descent(82/99): loss=207093.3698059733\n",
      "(250000,)\n",
      "Gradient Descent(83/99): loss=259300.5553183059\n",
      "(250000,)\n",
      "Gradient Descent(84/99): loss=260938.0770798551\n",
      "(250000,)\n",
      "Gradient Descent(85/99): loss=246782.9833052094\n",
      "(250000,)\n",
      "Gradient Descent(86/99): loss=164605.5424364004\n",
      "(250000,)\n",
      "Gradient Descent(87/99): loss=201649.3577224722\n",
      "(250000,)\n",
      "Gradient Descent(88/99): loss=158943.5357676825\n",
      "(250000,)\n",
      "Gradient Descent(89/99): loss=159934.7031944361\n",
      "(250000,)\n",
      "Gradient Descent(90/99): loss=160297.1720925902\n",
      "(250000,)\n",
      "Gradient Descent(91/99): loss=166874.8313077186\n",
      "(250000,)\n",
      "Gradient Descent(92/99): loss=171037.7922263116\n",
      "(250000,)\n",
      "Gradient Descent(93/99): loss=159179.3342928678\n",
      "(250000,)\n",
      "Gradient Descent(94/99): loss=162395.4940534979\n",
      "(250000,)\n",
      "Gradient Descent(95/99): loss=160346.2324611177\n",
      "(250000,)\n",
      "Gradient Descent(96/99): loss=168473.8812347844\n",
      "(250000,)\n",
      "Gradient Descent(97/99): loss=156437.4204706939\n",
      "(250000,)\n",
      "Gradient Descent(98/99): loss=161158.2087245676\n",
      "(250000,)\n",
      "Gradient Descent(99/99): loss=175870.6555590643\n"
     ]
    }
   ],
   "source": [
    "w_rlr, loss_rlr = reg_logistic_regression(y, tx, lambda_, initial_w, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
